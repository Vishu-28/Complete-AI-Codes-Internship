{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "129d9816",
   "metadata": {},
   "source": [
    "# Fine-tuning LLM with HuggingFace\n",
    "\n",
    "The model we are using is from `aboonaji` : https://huggingface.co/aboonaji/llama2finetune-v2\n",
    "\n",
    "The dataset we are using is from `aboonaji` :\n",
    "\n",
    "https://huggingface.co/datasets/aboonaji/wiki_medical_terms_llam2_format\n",
    "\n",
    "### Part 1. Installing and Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ba6e3a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: huggingface_hub in c:\\users\\public\\miniconda3\\envs\\trlenv\\lib\\site-packages (0.36.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\public\\miniconda3\\envs\\trlenv\\lib\\site-packages (from huggingface_hub) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\public\\miniconda3\\envs\\trlenv\\lib\\site-packages (from huggingface_hub) (2025.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\kasarla vishwaja\\appdata\\roaming\\python\\python310\\site-packages (from huggingface_hub) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\public\\miniconda3\\envs\\trlenv\\lib\\site-packages (from huggingface_hub) (6.0.3)\n",
      "Requirement already satisfied: requests in c:\\users\\public\\miniconda3\\envs\\trlenv\\lib\\site-packages (from huggingface_hub) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\public\\miniconda3\\envs\\trlenv\\lib\\site-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\kasarla vishwaja\\appdata\\roaming\\python\\python310\\site-packages (from huggingface_hub) (4.15.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\kasarla vishwaja\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>=4.42.1->huggingface_hub) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\public\\miniconda3\\envs\\trlenv\\lib\\site-packages (from requests->huggingface_hub) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\public\\miniconda3\\envs\\trlenv\\lib\\site-packages (from requests->huggingface_hub) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\public\\miniconda3\\envs\\trlenv\\lib\\site-packages (from requests->huggingface_hub) (2.6.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\public\\miniconda3\\envs\\trlenv\\lib\\site-packages (from requests->huggingface_hub) (2025.11.12)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# installing libraries \n",
    "%pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.7.10\n",
    "%pip install huggingface_hub "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef3076d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Public\\Miniconda3\\envs\\trlenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Public\\Miniconda3\\envs\\trlenv\\lib\\site-packages\\bitsandbytes\\cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1231 11:30:57.323000 20476 site-packages\\torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    }
   ],
   "source": [
    "# importing libraries\n",
    "import torch\n",
    "import scipy\n",
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bbd3ae",
   "metadata": {},
   "source": [
    "### Part 2. Building the AI\n",
    "\n",
    "#### loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbfb6e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Public\\Miniconda3\\envs\\trlenv\\lib\\site-packages\\huggingface_hub\\file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [05:45<00:00, 172.94s/it]\n"
     ]
    }
   ],
   "source": [
    "llama_model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=\"aboonaji/llama2finetune-v2\", torch_dtype=torch.float32, device_map={\"\": \"cpu\"},\n",
    "                                                   quantization_config = BitsAndBytesConfig(load_in_4bit=True,\n",
    "                                                                                            bnb_4bit_compute_dtype=getattr(torch, 'float16'),\n",
    "                                                                                            bnb_4bit_quant_type='nf4',))\n",
    "llama_model.config.use_cache = False\n",
    "llama_model.config.pretraining_tp=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2aea99d",
   "metadata": {},
   "source": [
    "#### loading the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "376fd818",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=\"aboonaji/llama2finetune-v2\",\n",
    "                                                trust_remote_code=True)\n",
    "llama_tokenizer.pad_token = llama_tokenizer.eos_token\n",
    "llama_tokenizer.padding_side = \"right\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3aea66",
   "metadata": {},
   "source": [
    "#### setting the training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "537ebed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llama-sft\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=10,\n",
    "\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    no_cuda=True,          \n",
    "    report_to=\"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408f1c54",
   "metadata": {},
   "source": [
    "### Part 3. Training the AI\n",
    "\n",
    "#### creating the supervised fine-tuning trainer (SFTTrainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2644988f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text']\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"aboonaji/wiki_medical_terms_llam2_format\",\n",
    "    split=\"train\"\n",
    ")\n",
    "\n",
    "print(dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "847e5e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bad rows: 0\n"
     ]
    }
   ],
   "source": [
    "# Count empty / bad rows\n",
    "bad_rows = dataset.filter(\n",
    "    lambda x: x[\"text\"] is None or len(x[\"text\"].strip()) == 0\n",
    ")\n",
    "\n",
    "print(\"Bad rows:\", len(bad_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2da25771",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_tokenizer.pad_token = llama_tokenizer.eos_token\n",
    "llama_tokenizer.padding_side = \"right\"\n",
    "llama_tokenizer.model_max_length = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39956770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trl: 0.7.10\n",
      "transformers: 4.31.0\n",
      "datasets: 4.4.2\n"
     ]
    }
   ],
   "source": [
    "import trl, transformers, datasets\n",
    "print(\"trl:\", trl.__version__)\n",
    "print(\"transformers:\", transformers.__version__)\n",
    "print(\"datasets:\", datasets.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac885685",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 6861/6861 [00:21<00:00, 326.14 examples/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mSFTTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllama_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllama_tokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_text_field\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpacking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLoraConfig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCAUSAL_LM\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlora_alpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlora_dropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Public\\Miniconda3\\envs\\trlenv\\lib\\site-packages\\trl\\trainer\\sft_trainer.py:295\u001b[0m, in \u001b[0;36mSFTTrainer.__init__\u001b[1;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics, peft_config, dataset_text_field, packing, formatting_func, max_seq_length, infinite, num_of_sequences, chars_per_token, dataset_num_proc, dataset_batch_size, neftune_noise_alpha, model_init_kwargs, dataset_kwargs)\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mpadding_side \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mpadding_side \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    290\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    291\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    292\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m` to your code.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    293\u001b[0m     )\n\u001b[1;32m--> 295\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_collator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    300\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    302\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    303\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    304\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    305\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    306\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreprocess_logits_for_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreprocess_logits_for_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    307\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mmax_steps \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m packing:\n\u001b[0;32m    310\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    311\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou passed `packing=True` to the SFTTrainer, and you are training your model with `max_steps` strategy. The dataset will be iterated until the `max_steps` are reached.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    312\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Public\\Miniconda3\\envs\\trlenv\\lib\\site-packages\\transformers\\trainer.py:383\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[1;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_model_parallel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 383\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_model_parallel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[43mdevices\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m    385\u001b[0m \u001b[38;5;66;03m# warn users\u001b[39;00m\n\u001b[0;32m    386\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[0;32m    387\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have loaded a model on multiple GPUs. `is_model_parallel` attribute will be force-set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m to `True` to avoid any unexpected behavior such as device placement mismatching.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    389\u001b[0m )\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=llama_model,\n",
    "    tokenizer=llama_tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=512,\n",
    "    packing=False,\n",
    "    peft_config=LoraConfig(\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        r=64,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16da3745",
   "metadata": {},
   "source": [
    "#### training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17722804",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9096fd7a",
   "metadata": {},
   "source": [
    "API KEY: sign in, cpoy api key, then the above line will run completely and the training starts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9189d17b",
   "metadata": {},
   "source": [
    "### Part 4. Chatting with AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6531ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = \"Tell me about Paracetamol Poising\"\n",
    "text_gen_pipeline = pipeline(task = \"text-generation\", model = llama_model, tokenizer = llama_tokenizer, max_length = 300)\n",
    "model_answer = text_gen_pipeline(f\"<s>[INST]{user_prompt}[/ISNT]\")\n",
    "print(model_answer[0]['generated_text']) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d8c371",
   "metadata": {},
   "source": [
    "next search for Bursitis inplace of Paracetamol Poising"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
